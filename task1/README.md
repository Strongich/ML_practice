# Task 1. Natural Language Processing. Named entity recognition
In this task, we need to train a named entity recognition (NER) model for the identification of
mountain names inside the texts. For this purpose you need:
* Find / create a dataset with labeled mountains.
* Select the relevant architecture of the model for NER solving.
* Train / finetune the model.
* Prepare demo code / notebook of the inference results.

## Dataset 
Using recommendations, I decided to create dataset with ChatGPT-3.5. All steps of creating I described in <b>data_creation.ipynb</b>.
## Model selection
For Name Entity Recognition (NER) task for the identification of mountain names inside text i've used fine-tuned BERT, that was trained on standart <b>CoNLL-2003 Named Entity Recognition</b> dataset. You can find original model [here](https://huggingface.co/dslim/bert-base-NER).
## Fine-tuning 
Before talking about fine-tuning, I want to indicate that from original model I've deleted 2 last layers because of lack of video memmory on my GPU. 

Our pretrained model had other labels from ours. So in order to specify this model for our task I've trained it on generated by ChatGPT dataset, previously tokenized it. I used AdamW optimizer with learning rate=0.00002, 5 epochs, batch size 4 and weight decay 0.01. Results is:
* eval_f1: 0.897
* eval_accuracy: 0.97 \
All weights of model are stored in <b>ner_model</b> folder.\
All steps of tokenizing input data and training I've described in <b>train.py</b>.
## Setup
To use trained model, follow the instructions below:
1. First clone the repository. To do this, open a terminal, go to the directory where you want to clone the project and then enter the command:
```bash
git clone https://github.com/Strongich/ds_intership.git
```
2. Go to folder with project and this task and install virtualenv, write the following command and press Enter:
```bash
cd task1
pip install virtualenv
```
3. Next create a new environment, write the following command and press Enter:
```bash
virtualenv name_of_the_new_env
```
### Example:
```bash
virtualenv ner
```
4. Next activate the new environment, write the following command and press Enter:
```bash
name_of_the_new_env\Scripts\activate
```
### Example:
```bash
ner\Scripts\activate
```
5. Write the following command and press Enter:
 ```bash
pip install -r requirements.txt
```
6. You can now open <b>inference_demo.ipynb</b> notebook and use it, OR write it in console and press Enter:
```bash
python
from inference import run_inference
example = "this is inference test via terminal"
run_inference(example)
```
You can change the sentence "this is inference test..." to your own to try it.

## Improvements
* Due to lack of video memmory, i wasn't using last 2 layers of model. So in order to get better results we can quantize original pretrained model or use stronger GPU.
* My dataset was very tinny, we can spend more time to create much more examples and use some augmentation techniques like back translation, synonym replacement, random insertion etc. In combination with first point it should make model really strong for this type of task. Also, the training examples wasn't very complicate, so in order to be ready for more complex texts maybe it will be usefull to write prompt for ChatGPT in more specific way - if the target texts is fiction, add this spec to it and so on.
* At the moment using inference is not very comfortable, so in future it can be optimized for better user experience, such as:
    * adding UI with backend request
    * running it on .txt files and .pdf files (pdf parser needed)
    * make output in the same format, as input, but with labeled mountains
